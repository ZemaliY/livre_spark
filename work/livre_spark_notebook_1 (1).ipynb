{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Chargement de la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = \"./../data/Villes/ville_1.csv\"\n",
    "ville = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " id                 | 1                    \n",
      " vitesse_a_pied     | 0.12491350287937626  \n",
      " vitesse_a_velo     | 0.33310267434500335  \n",
      " home               | (lon:30.28 lat:19... \n",
      " travail            | (lon:27.39 lat:15... \n",
      " sportif            | False                \n",
      " casseur            | False                \n",
      " statut             | technicien_de_sur... \n",
      " salaire            | 20326.98056600918    \n",
      " sexe               | H                    \n",
      " age                | 73                   \n",
      " sportivite         | 0.4163783429312542   \n",
      " velo_perf_minimale | 0.4                  \n",
      "-RECORD 1----------------------------------\n",
      " id                 | 2                    \n",
      " vitesse_a_pied     | 4.85694978536844     \n",
      " vitesse_a_velo     | 12.951866094315841   \n",
      " home               | (lon:22.33 lat:25... \n",
      " travail            | (lon:2.45 lat:17.53) \n",
      " sportif            | True                 \n",
      " casseur            | False                \n",
      " statut             | employe              \n",
      " salaire            | 13730.356092078828   \n",
      " sexe               | H                    \n",
      " age                | 40                   \n",
      " sportivite         | 5.3966108726316      \n",
      " velo_perf_minimale | 5.7966108726316      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ville.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1) types des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('vitesse_a_pied', 'string'),\n",
       " ('vitesse_a_velo', 'string'),\n",
       " ('home', 'string'),\n",
       " ('travail', 'string'),\n",
       " ('sportif', 'string'),\n",
       " ('casseur', 'string'),\n",
       " ('statut', 'string'),\n",
       " ('salaire', 'string'),\n",
       " ('sexe', 'string'),\n",
       " ('age', 'string'),\n",
       " ('sportivite', 'string'),\n",
       " ('velo_perf_minimale', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) décompte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='1', vitesse_a_pied='0.12491350287937626', vitesse_a_velo='0.33310267434500335', home='(lon:30.28 lat:19.95)', travail='(lon:27.39 lat:15.92)', sportif='False', casseur='False', statut='technicien_de_surface', salaire='20326.98056600918', sexe='H', age='73', sportivite='0.4163783429312542', velo_perf_minimale='0.4')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) filtre, nombre d'hommes : where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.where(ville.sexe ==\"H\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) requête sql : createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', count(1)=556), Row(sexe='H', count(1)=493)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# référencement de la dataFame commetable SQL\n",
    "ville.createOrReplaceTempView(\"ville\")\n",
    "# requête sql\n",
    "requete_sql = \"select sexe, count(*) from ville group by sexe\"\n",
    "# utilisation de la variable spark pour appliquer la requête sql\n",
    "spark.sql(requete_sql).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1) méthodes de l'objet dataFrame avec des équivalents SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', count=556), Row(sexe='H', count=493)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ville.groupBy(\"sexe\").count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) moyenne des salaires par sexe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1) Gérer les types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'\"salaire\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o276.mean.\n: org.apache.spark.sql.AnalysisException: \"salaire\" is not a numeric column. Aggregation function can only be applied on a numeric column.;\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$4.apply(RelationalGroupedDataset.scala:101)\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$4.apply(RelationalGroupedDataset.scala:98)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:98)\n\tat org.apache.spark.sql.RelationalGroupedDataset.mean(RelationalGroupedDataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-163cedbee4e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cette ligne ne fonctionne pas : il faut transformer le salaire en numérique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mville\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sexe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salaire\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: '\"salaire\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'"
     ]
    }
   ],
   "source": [
    "# cette ligne ne fonctionne pas : il faut transformer le salaire en numérique\n",
    "ville.groupBy([\"sexe\"]).mean(\"salaire\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2) Spark transforme les string en double via le SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', avg(CAST(salaire AS DOUBLE))=23419.003849651763),\n",
       " Row(sexe='H', avg(CAST(salaire AS DOUBLE))=28727.584395358714)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requete_sql = \"select sexe, mean(salaire) from ville group by sexe\"\n",
    "spark.sql(requete_sql).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3) Spark propose des fonctions qui transforme aussi nativement les string en nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', avg(salaire)=23419.003849651763),\n",
       " Row(sexe='H', avg(salaire)=28727.584395358714)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, first\n",
    "ville.groupBy([\"sexe\"]).agg(avg(\"salaire\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4) sinon il faut gérer à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', avg(salaire_float)=23419.003849651763),\n",
       " Row(sexe='H', avg(salaire_float)=28727.584395358714)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on importe le type DoubleType\n",
    "from pyspark.sql.types import DoubleType\n",
    "# on rajoute une colonne \"salaire_float\" qui vaut la colonne \"salaire\" changée en DoubleType\n",
    "ville = ville.withColumn(\"salaire_float\", ville.salaire.cast(DoubleType() ))\n",
    "# on peut ensuite calculer la moyenne sur cette nouvelle colonne \"salaire_float\"\n",
    "ville.groupBy([\"sexe\"]).mean(\"salaire_float\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4) trouver le nombre de personne par tranche de salaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorie(34000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " id                 | 1                    \n",
      " vitesse_a_pied     | 0.12491350287937626  \n",
      " vitesse_a_velo     | 0.33310267434500335  \n",
      " home               | (lon:30.28 lat:19... \n",
      " travail            | (lon:27.39 lat:15... \n",
      " sportif            | False                \n",
      " casseur            | False                \n",
      " statut             | technicien_de_sur... \n",
      " salaire            | 20326.98056600918    \n",
      " sexe               | H                    \n",
      " age                | 73                   \n",
      " sportivite         | 0.4163783429312542   \n",
      " velo_perf_minimale | 0.4                  \n",
      " salaire_float      | 20326.98056600918    \n",
      " salaire_categorie  | 20000                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def categorie(salaire):\n",
    "    \"\"\"\n",
    "    Calcule la dizaine de millier dans lequel se trouve le salaire.\n",
    "    Ex : \n",
    "        15000 -> 10000\n",
    "        34000 -> 30000\n",
    "    \"\"\"\n",
    "    nb_de_dizaine_de_millier = float(salaire)//10000\n",
    "    categorie                = 10000 * nb_de_dizaine_de_millier\n",
    "    return  int(categorie)\n",
    "\n",
    "# import de la fonction \"udf\" permettant d'enregistrer des fonctions définies par nous même, applicable sur des colonnes \n",
    "from pyspark.sql.functions import udf\n",
    "# import des types sql, pour pouvoir type le résultat de notre fonction (Spark est codé en java, qui est un langage très typé)\n",
    "from pyspark.sql.types     import *\n",
    "# enregistrement de notre fonction comme une fonction udf\n",
    "ma_fonction = udf(categorie , IntegerType())\n",
    "# application de notre fonction udf sur la colonne salaire, et enregistrement du résultat dans une nouvelle colonne \"salaire_categorie\"\n",
    "ville = ville.withColumn(\"salaire_categorie\", ma_fonction(\"salaire\"))\n",
    "ville.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', salaire_categorie=80000, count=1),\n",
       " Row(sexe='H', salaire_categorie=70000, count=2),\n",
       " Row(sexe='H', salaire_categorie=80000, count=2),\n",
       " Row(sexe='F', salaire_categorie=70000, count=3),\n",
       " Row(sexe='F', salaire_categorie=50000, count=4),\n",
       " Row(sexe='F', salaire_categorie=0, count=5),\n",
       " Row(sexe='H', salaire_categorie=60000, count=7),\n",
       " Row(sexe='H', salaire_categorie=50000, count=15),\n",
       " Row(sexe='F', salaire_categorie=40000, count=20),\n",
       " Row(sexe='H', salaire_categorie=40000, count=49),\n",
       " Row(sexe='F', salaire_categorie=30000, count=84),\n",
       " Row(sexe='H', salaire_categorie=30000, count=104),\n",
       " Row(sexe='H', salaire_categorie=10000, count=106),\n",
       " Row(sexe='F', salaire_categorie=20000, count=205),\n",
       " Row(sexe='H', salaire_categorie=20000, count=208),\n",
       " Row(sexe='F', salaire_categorie=10000, count=234)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_par_tranche =ville.groupBy([\"sexe\", \"salaire_categorie\"]).count()\n",
    "pop_par_tranche.orderBy(\"count\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sexe='F', salaire_categorie=0, count=5),\n",
       " Row(sexe='F', salaire_categorie=10000, count=234),\n",
       " Row(sexe='F', salaire_categorie=20000, count=205),\n",
       " Row(sexe='F', salaire_categorie=30000, count=84),\n",
       " Row(sexe='F', salaire_categorie=40000, count=20),\n",
       " Row(sexe='F', salaire_categorie=50000, count=4),\n",
       " Row(sexe='F', salaire_categorie=70000, count=3),\n",
       " Row(sexe='F', salaire_categorie=80000, count=1),\n",
       " Row(sexe='H', salaire_categorie=10000, count=106),\n",
       " Row(sexe='H', salaire_categorie=20000, count=208),\n",
       " Row(sexe='H', salaire_categorie=30000, count=104),\n",
       " Row(sexe='H', salaire_categorie=40000, count=49),\n",
       " Row(sexe='H', salaire_categorie=50000, count=15),\n",
       " Row(sexe='H', salaire_categorie=60000, count=7),\n",
       " Row(sexe='H', salaire_categorie=70000, count=2),\n",
       " Row(sexe='H', salaire_categorie=80000, count=2)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ville.groupBy([\"sexe\", \"salaire_categorie\"]).count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Group by : calcul de nombre d'élément par groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|sexe|size(collect_list(sexe))|\n",
      "+----+------------------------+\n",
      "|   F|                     556|\n",
      "|   H|                     493|\n",
      "+----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "ville.groupBy(\"sexe\").agg(func.size(func.collect_list('sexe'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6) Group by : calcul de nombre d'élément distincts par groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------+\n",
      "|sexe|size(collect_set(salaire_categorie))|\n",
      "+----+------------------------------------+\n",
      "|   F|                                   8|\n",
      "|   H|                                   8|\n",
      "+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ville.groupBy(\"sexe\").agg(func.size(func.collect_set('salaire_categorie'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = \"./../data/Cyclistes/cycliste_1.csv\" \n",
    "cycliste_1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1', timestamp='1', sur_velo='False', velo='False', vitesse='0.12491350287937626', position='(lon:30.28 lat:19.95)', destination_finale='(lon:27.39 lat:15.92)'),\n",
       " Row(id='1', timestamp='2', sur_velo='False', velo='False', vitesse='0.12491350287937626', position='(lon:30.16 lat:19.97)', destination_finale='(lon:27.39 lat:15.92)'),\n",
       " Row(id='1', timestamp='3', sur_velo='False', velo='False', vitesse='0.12491350287937626', position='(lon:30.03 lat:19.99)', destination_finale='(lon:27.39 lat:15.92)')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycliste_1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycliste_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = \"./../data/Cyclistes/*.csv\" \n",
    "tous_les_cyclistes = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tous_les_cyclistes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'timestamp',\n",
       " 'sur_velo',\n",
       " 'velo',\n",
       " 'vitesse',\n",
       " 'position',\n",
       " 'destination_finale']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tous_les_cyclistes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
